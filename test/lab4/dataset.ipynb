{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0419 16:44:58.152588 60 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.276594 60 compiler.py:951] Jittor(1.3.2.5) src: /opt/miniconda/envs/dhy/lib/python3.9/site-packages/jittor\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.284781 60 compiler.py:952] g++ at /usr/bin/g++(9.3.0)\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.286829 60 compiler.py:953] cache_path: /home/dhy/.cache/jittor/jt1.3.2/g++9.3.0/py3.9.7/Linux-3.10.0-1xeb/IntelRXeonRGolx19/default\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.301499 60 install_cuda.py:51] cuda_driver_version: [11, 4]\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.309968 60 __init__.py:411] Found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc(11.2.152) at /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc.\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.360354 60 __init__.py:411] Found gdb(9.2) at /usr/bin/gdb.\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.368725 60 __init__.py:411] Found addr2line(2.34) at /usr/bin/addr2line.\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.514514 60 compiler.py:1006] cuda key:cu11.2.152_sm_86\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.736182 60 __init__.py:227] Total mem: 251.43GB, using 16 procs for compiling.\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:58.917973 60 jit_compiler.cc:28] Load cc_path: /usr/bin/g++\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:59.071279 60 init.cc:62] Found cuda archs: [86,]\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:59.100552 60 compile_extern.py:516] mpicc not found, distribution disabled.\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:59.168724 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cublas.h\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:59.181585 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcublas.so\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:44:59.182459 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcublasLt.so.11\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:00.493496 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cudnn.h\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:00.551147 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:00.552271 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_ops_infer.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:00.558593 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_ops_train.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:00.560129 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_cnn_infer.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:00.593661 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_cnn_train.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:01.936734 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/curand.h\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:01.993297 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcurand.so\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:02.012928 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cufft.h\u001b[m\n",
      "\u001b[38;5;2m[i 0419 16:45:02.078617 60 compile_extern.py:30] found /home/dhy/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcufft.so\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jittor as jt\n",
    "import os\n",
    "from os.path import join, getsize\n",
    "\n",
    "from jittor.dataset import Dataset\n",
    "from PIL import Image\n",
    "from dataset import Tiny_vid\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jittor import nn\n",
    "import jittor.transform as trans\n",
    "import numpy as np\n",
    "from jittor.models.resnet import Resnet50, ResNet, Resnet34\n",
    "from dataset import Tiny_vid\n",
    "from argparse import ArgumentParser\n",
    "from bbox import box_iou_batch\n",
    "jt.misc.set_global_seed(425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_to_rect(bbox, color):\n",
    "    # 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：\n",
    "    # ((左上x,左上y),宽,高)\n",
    "    return plt.Rectangle(\n",
    "        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\n",
    "        fill=False, edgecolor=color, linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Tiny_vid(train=False).set_attrs(batch_size=1, shuffle=True)\n",
    "# class_dict = {'bird':0.,'car':1.,'dog':2.,'lizard':3.,'turtle':4.}\n",
    "# from IPython import display\n",
    "# for x, y in dataset:\n",
    "#     plt.figure()\n",
    "#     ax = plt.imshow(x.squeeze(0))\n",
    "#     test_bbox = np.floor(y[1]*128)\n",
    "#     test_bbox = test_bbox.squeeze(0).tolist()\n",
    "#     ax.axes.add_patch(bbox_to_rect(test_bbox, 'blue'))\n",
    "#     plt.title(f\"'bird':0.,'car':1.,'dog':2.,'lizard':3.,'turtle':4.class:{y[0]}\")    \n",
    "#     plt.show()\n",
    "#     a = input(\"aa:\")\n",
    "#     if a == '1':\n",
    "#         display.clear_output(wait=True)\n",
    "#         continue\n",
    "#     if a == '2':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"计算两个锚框或边界框列表中成对的交并比\"\"\"\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n",
    "                              (boxes[:, 3] - boxes[:, 1]))\n",
    "    # boxes1,boxes2,areas1,areas2的形状:\n",
    "    # boxes1：(boxes1的数量,4),\n",
    "    # boxes2：(boxes2的数量,4),\n",
    "    # areas1：(boxes1的数量,),\n",
    "    # areas2：(boxes2的数量,)\n",
    "    areas1 = box_area(boxes1)\n",
    "    areas2 = box_area(boxes2)\n",
    "    import pdb;pdb.set_trace()\n",
    "\n",
    "    # inter_upperlefts,inter_lowerrights,inters的形状:\n",
    "    # (boxes1的数量,boxes2的数量,2)\n",
    "    inter_upperlefts = np.maximum(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    inter_lowerrights = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clip(min=0)\n",
    "    # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
    "    return inter_areas / union_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DetNet, self).__init__()\n",
    "        self.preprocess = nn.Sequential(\n",
    "            nn.Relu(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            \n",
    "        )\n",
    "        # self.backbone = Resnet50(num_classes=5, pretrained=True)\n",
    "        # self.backbone = Resnet50(pretrained=True)\n",
    "        resnet = Resnet34(pretrained=True)\n",
    "        layers = list(resnet.children())[:8]\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512,5)\n",
    "        )\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512,4)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def execute(self, x):\n",
    "        # pdb.set_trace()\n",
    "        # with jt.no_grad():\n",
    "        y = self.backbone(x)\n",
    "        y = self.preprocess(y)\n",
    "        y =  y.view(y.shape[0], -1)\n",
    "        class_score = self.class_head(y)\n",
    "        bbox_out = self.bbox_head(y)\n",
    "        return class_score, bbox_out\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        outputs, outboxes = model(inputs)\n",
    "        with jt.no_grad():\n",
    "            batch_size = inputs.shape[0]\n",
    "            pred,_ = jt.argmax(outputs.data, dim=1)\n",
    "            class_acc = jt.sum(targets[0] == pred) / batch_size\n",
    "            iou = box_iou_batch(outboxes, targets[1])\n",
    "            all_correct = jt.sum((iou > 0.5)*(targets[0] == pred))\n",
    "        loss_b = nn.l1_loss(outboxes, targets[1])\n",
    "        # loss_b = loss_b.sum()\n",
    "        loss_c = nn.cross_entropy_loss(outputs, targets[0], reduction=\"mean\")\n",
    "        # pdb.set_trace()\n",
    "        loss = loss_c+ loss_b\n",
    "        optimizer.step (loss)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tClass Acc: {:.6f}\\tBox_Acc: {:.6f}\\tAll Acc: {:.6f}'.format(\n",
    "                    epoch, batch_idx, len(train_loader),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0], class_acc, jt.sum(iou > 0.5)/batch_size, all_correct/batch_size))\n",
    "\n",
    "\n",
    "\n",
    "def train_class_head(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    # freeze bbox head\n",
    "    for name, param in model.class_head.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    for name, param in model.backbone.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    for name, param in model.preprocess.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    for name, param in model.bbox_head.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        outputs, outboxes = model(inputs)\n",
    "        with jt.no_grad():\n",
    "            batch_size = inputs.shape[0]\n",
    "            pred,_ = jt.argmax(outputs.data, dim=1)\n",
    "            class_acc = jt.sum(targets[0] == pred) / batch_size\n",
    "        loss = nn.cross_entropy_loss(outputs, targets[0])\n",
    "        optimizer.step (loss)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTrain Acc: {:.6f}'.format(\n",
    "                    epoch, batch_idx, len(train_loader),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0], class_acc))\n",
    "\n",
    "\n",
    "def train_bbox_head(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    # freeze preprocess and class head\n",
    "    for name, param in model.class_head.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, param in model.backbone.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, param in model.preprocess.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, param in model.bbox_head.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # pdb.set_trace()\n",
    "        batch_size = inputs.shape[0]\n",
    "        outputs, outboxes = model(inputs)\n",
    "        # loss = nn.mse_loss(outboxes, targets[1])\n",
    "        loss = nn.l1_loss(outboxes, targets[1])\n",
    "        iou = box_iou_batch(outboxes, targets[1])\n",
    "        optimizer.step (loss)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(' BOX Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tBox_Acc: {:.6f}'.format(\n",
    "                    epoch, batch_idx, len(train_loader),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0], jt.sum(iou > 0.5)/batch_size))\n",
    "\n",
    "\n",
    "total_acc = 0\n",
    "total_num = 0\n",
    "def test(model, val_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        batch_size = inputs.shape[0]\n",
    "        outputs, outboxes = model(inputs)\n",
    "        pred,_ = jt.argmax(outputs.data, dim=1)\n",
    "        # pdb.set_trace()\n",
    "        class_acc = jt.sum(targets[0] == pred)\n",
    "        test_iou = box_iou_batch(outboxes, targets[1])\n",
    "        all_correct = jt.sum((test_iou > 0.5)*(targets[0] == pred))\n",
    "        global total_acc\n",
    "        global total_num \n",
    "        total_acc += class_acc\n",
    "        total_num += batch_size\n",
    "        \n",
    "    print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tClass Acc: {:.6f}\\tBoth Acc: {:.6f}\\tBox_Acc: {:.6f}'.format(epoch, \\\n",
    "                batch_idx, len(val_loader),100. * float(batch_idx) / len(val_loader), class_acc/ batch_size, all_correct/batch_size, jt.sum(test_iou > 0.5)/batch_size))\n",
    "    return class_acc    \n",
    "\n",
    "def param_dict():\n",
    "    parser = ArgumentParser(description=\"Hyper parameters\")\n",
    "    parser.add_argument('-b','--batch_size', default=16, type=int)\n",
    "    parser.add_argument('-l','--learning_rate', default=1e-2, type=float)\n",
    "    parser.add_argument('-e','--epochs', default=200, type=int)\n",
    "    args = parser.parse_args()\n",
    "    return vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break: ./tiny_vid/bird    000151.JPEG\n",
      "break: ./tiny_vid/car    000151.JPEG\n",
      "break: ./tiny_vid/dog    000151.JPEG\n",
      "break: ./tiny_vid/lizard    000151.JPEG\n",
      "break: ./tiny_vid/turtle    000151.JPEG\n",
      "dataset img len: 1500\n",
      "dataset label len: 1500\n",
      "break: ./tiny_vid/bird    000181.JPEG\n",
      "break: ./tiny_vid/car    000181.JPEG\n",
      "break: ./tiny_vid/dog    000181.JPEG\n",
      "dataset img len: 150\n",
      "dataset label len: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling Operators(19/19) used: 2.31s eta:    0s \n",
      "\n",
      "Compiling Operators(7/7) used: 2.31s eta:    0s \n",
      "\n",
      "Compiling Operators(3/3) used: 2.31s eta:    0s \n",
      "\n",
      "Compiling Operators(4/4) used: 2.31s eta:    0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/94 (0%)]\tLoss: 2.890447\tClass Acc: 0.125000\tBox_Acc: 0.000000\tAll Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling Operators(43/43) used: 2.32s eta:    0s \r"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 1e-2\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "my_transform = trans.Compose([\n",
    "    trans.ToTensor()\n",
    "    ])\n",
    "\n",
    "train_loader = Tiny_vid(train=True, transform=my_transform, aug=True).set_attrs(batch_size=batch_size, shuffle=True)\n",
    "val_loader = Tiny_vid(train=False, transform=my_transform)\n",
    "val_loader.set_attrs(batch_size=len(val_loader.ground_truth), shuffle=False)\n",
    "\n",
    "model = DetNet()\n",
    "optimizer = nn.SGD(list(filter(lambda val: val.requires_grad, model.parameters())), learning_rate, momentum, weight_decay)\n",
    "optimizer_box = nn.SGD(list(filter(lambda val: val.requires_grad, model.parameters())), learning_rate, momentum, weight_decay)\n",
    "for epoch in range(epochs):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, val_loader, epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc8d0c2ba487628ef9539a9907cd300ddff1381c4c1f4ed73c440a978aa681e3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dhy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
